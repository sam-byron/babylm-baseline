{
  "_comment": "LTG-BERT (BERT-base) pretraining config tuned for 3x RTX 5090 32GB, fixed seq_len=128 (first 90% of paper)",
  "_source": "Trained on 100 million words and still in shape: BERT meets British National Corpus (arXiv:2303.09859)",

  "block_size": 128,
  "num_epochs": 225,

  "_batching_notes": "Per-GPU batch_size * grad_accum * world_size = effective sequences per optimizer update",
  "batch_size": 48,
  "grad_accum": 64,
  "_world_size_hint": 3,
  "_effective_sequences_per_update": 9216,
  "_effective_tokens_per_update": 1179648,

  "vocab_size": 16384,
  "dataset_split": "train",
  "tokenizer_path": "./data/pretrain/wordpiece_vocab.json",

  "_optimizer": "Use AdamW with sub-linear LR scaling; LAMB path in trainer has scheduler coupling hazards",
  "optimizer": "adamw",
  "use_fused_adamw": true,
  "learning_rate": 0.0005,
  "lr_scale_alpha": 0.5,
  "peak_lr_cap": 0.003,
  "min_lr": 1e-5,
  "warmup_steps_proportion": 0.02,
  "optimizer_eps": 1e-8,
  "weight_decay": 0.01,
  "head_weight_decay": 0.1,
  "max_grad_norm": 1.0,

  "_dropout": "Paper uses 0.1 for both",
  "attention_probs_dropout_prob": 0.1,
  "hidden_dropout_prob": 0.1,

  "_model_dims": "BERT-base-like with LTG-BERT specifics",
  "hidden_size": 768,
  "intermediate_size": 2048,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "position_bucket_size": 32,
  "layer_norm_eps": 1e-5,
  "share_layer_weights": true,

  "_masking": "Whole-word masking with dynamic masking enabled",
  "masking_strategy": "whole_word",
  "mask_p": 0.15,
  "random_p": 0.1,
  "keep_p": 0.1,
  "max_span_length": 5,
  "use_dynamic_masking": true,

  "checkpoint_path": "model_babylm_bert_ltg_checkpoint",
  "cache_path": "model_babylm_bert_ltg",
  "chunk_size": 10000,

  "monitoring": {
    "spike_threshold": 0.6,
    "oscillation_threshold": 0.3,
    "window_size": 100
  },

  "_inference": {
    "max_new_tokens": 400,
    "max_response_length": 150,
    "temperature": 0.9,
    "top_p": 0.8
  }
}