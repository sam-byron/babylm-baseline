{
    "block_size": 512,
        "batch_size": 32,
        "grad_accum": 8,
    "num_epochs": 30,
    "vocab_size": 16384,
    "dataset_split": "train",
    "learning_rate": 1e-4,
    "weight_decay": 0.01,
    "head_weight_decay": 0.01,
    "warmup_steps_proportion": 0.06,
    "checkpoint_path": "model_babylm_bert_ltg_checkpoint",
    "cache_path": "model_babylm_bert_ltg",
    "max_new_tokens": 400,
    "max_response_length": 150,
    "temperature": 0.9,
    "top_p": 0.8,
    "chunk_size": 10000,
    "max_grad_norm": 1.5,
    "tokenizer_path": "./data/pretrain/wordpiece_vocab.json",
    "attention_probs_dropout_prob": 0.05,
    "hidden_dropout_prob": 0.05,
    "hidden_size": 768,
    "intermediate_size": 2048,
    "position_bucket_size": 32,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "layer_norm_eps": 1e-5,
    "masking_strategy": "span",
    "mask_p": 0.15,
    "random_p": 0.1,
    "keep_p": 0.1,
    "use_dynamic_masking": true,
    "optimizer": "adamw",
    "share_layer_weights": false,
    "monitoring": {
        "spike_threshold": 0.3,
        "oscillation_threshold": 0.15,
        "window_size": 20
    }
}
