{
    "block_size": 512,
    "batch_size": 4,
    "grad_accum": 24,
    "num_epochs": 50,
    "vocab_size": 16384,
    "dataset_split": "train",
    "learning_rate": 5e-06,
    "weight_decay": 0.0001,
    "head_weight_decay": 0.01,
    "warmup_steps_proportion": 0.3,
    "checkpoint_path": "model_babylm_bert_ltg_checkpoint",
    "cache_path": "model_babylm_bert_ltg",
    "max_new_tokens": 400,
    "max_response_length": 150,
    "temperature": 0.9,
    "top_p": 0.8,
    "chunk_size": 40,
    "max_grad_norm": 0.1,
    "tokenizer_path": "./data/pretrain/wordpiece_vocab.json",
    "attention_probs_dropout_prob": 0.1,
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "intermediate_size": 2048,
    "max_position_embeddings": 512,
    "position_bucket_size": 32,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "layer_norm_eps": 1e-05,
    "masking_strategy": "span",
    "mask_p": 0.15,
    "random_p": 0.1,
    "keep_p": 0.1,
    "optimizer": "adamw",
    "monitoring": {
        "spike_threshold": 0.3,
        "oscillation_threshold": 0.15,
        "window_size": 20
    }
}