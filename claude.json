{
    "_comment": "LTG-BERT configuration optimized for 3x RTX 5090 GPUs (96GB total VRAM)",
    "_paper_reference": "Trained on 100 million words and still in shape: BERT meets British National Corpus (arXiv:2303.09859)",
    "_optimization_notes": "Progressive sequence length, LAMB optimizer, layer sharing, massive batch training",
    
    "block_size": 128,
    "_sequence_length_note": "Fixed at 128 tokens (90% of paper's progressive strategy, no dynamic switching)",
    
    "_batch_calculation": "batch_size * grad_accum = 32 * 128 = 4096 sequences = 524,288 tokens per batch",
    "batch_size": 32,
    "grad_accum": 128,
    "effective_batch_size_tokens": 524288,
    
    "num_epochs": 225,
    "dataset_repetition_factor": 250,
    "_token_exposure_note": "Each token seen 250x as per LTG-BERT paper methodology",
    
    "vocab_size": 16384,
    "dataset_split": "train",
    
    "_learning_rate_scaling": "sqrt(batch_size / 256) * base_lr for LAMB optimizer",
    "learning_rate": 8e-3,
    "peak_lr_cap": 2e-2,
    "min_lr": 1e-6,
    "lr_scale_alpha": 0.5,
    "warmup_steps_proportion": 0.02,
    
    "_optimizer_settings": "LAMB for large batch training stability",
    "optimizer": "lamb",
    "optimizer_eps": 1e-6,
    "weight_decay": 0.01,
    "head_weight_decay": 0.1,
    "max_grad_norm": 5.0,
    
    "_ltg_bert_architecture": "Key innovations from paper",
    "share_layer_weights": false,
    "hidden_size": 768,
    "intermediate_size": 2048,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "position_bucket_size": 32,
    "layer_norm_eps": 1e-5,
    
    "_regularization_optimized": "Paper uses higher dropout for generalization",
    "attention_probs_dropout_prob": 0.1,
    "hidden_dropout_prob": 0.1,
    
    "_masking_strategy": "Whole word masking as per paper",
    "masking_strategy": "whole_word",
    "mask_p": 0.15,
    "random_p": 0.1,
    "keep_p": 0.1,
    "max_span_length": 5,
    "use_dynamic_masking": true,
    
    "_multi_gpu_settings": "Optimized for 3x RTX 5090",
    "use_distributed_training": true,
    "gradient_checkpointing": false,
    "mixed_precision": "bf16",
    "dataloader_num_workers": 8,
    
    "_checkpoint_management": "Optimized for 96GB VRAM",
    "checkpoint_path": "model_babylm_bert_ltg_checkpoint",
    "cache_path": "model_babylm_bert_ltg",
    "save_every_n_epochs": 25,
    "keep_best_n_checkpoints": 3,
    
    "_inference_settings": "Standard generation parameters",
    "max_new_tokens": 400,
    "max_response_length": 150,
    "temperature": 0.9,
    "top_p": 0.8,
    
    "_data_processing": "Chunk size optimized for memory",
    "chunk_size": 50000,
    "tokenizer_path": "./data/pretrain/wordpiece_vocab.json",
    
    "_monitoring_and_stability": "Adjusted for large batch training",
    "monitoring": {
        "spike_threshold": 1.0,
        "oscillation_threshold": 0.5,
        "window_size": 100,
        "log_every_n_steps": 10,
        "eval_every_n_epochs": 10
    },
    
    "_hardware_optimization": "RTX 5090 specific optimizations",
    "compile_model": true,
    "use_flash_attention": true,
    "tensor_parallel_size": 3,
    "zero_stage": 2,
    
    "_memory_optimization": "96GB total VRAM utilization",
    "max_memory_per_gpu": "30GB",
    "gradient_accumulation_steps": 128,
    "per_device_train_batch_size": 32,
    "per_device_eval_batch_size": 16,
    
    "_computational_efficiency": "Using 90% of paper's progressive sequence strategy (fixed 128 tokens)",
    "estimated_training_time_hours": 48,
    "estimated_tokens_per_batch": 524288,
    
    "_paper_fidelity_check": {
        "fixed_sequences_128": "✓ First 90% of paper strategy",
        "layer_sharing": "✓ Enabled", 
        "lamb_optimizer": "✓ Large batch scaling",
        "token_repetition": "✓ 250x exposure",
        "whole_word_masking": "✓ Enabled",
        "massive_batches": "✓ 524K tokens per batch"
    }
}